{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Importing Classifier Modules\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# We do this to ignore several specific Pandas warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/UL494YB/OneDrive - EY/Desktop/Uchicago/Data Mining Principle/session 5/Assignment 3/Assignment_3/Loan_Prediction/Data for Cleaning & Modeling.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning/Prepping - Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#based on my analysis, I decided to use the following features for my training data\n",
    "df_reduced=df[['X1','X4','X5','X7','X8','X11',\n",
    "        'X13', 'X14','X17', 'X21',\n",
    "       'X22','X24', 'X27', 'X28', 'X29', 'X30', 'X31',\n",
    "       'X32']]\n",
    "df_reduced.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_reduced['X1'].unique()))\n",
    "df_reduced['X1'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X1']=df_reduced['X1'].replace({'%':''}, regex = True)\n",
    "df_reduced['X1']=df_reduced['X1'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X1'].describe()\n",
    "#getting summary stats on my target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X4']=df_reduced['X4'].replace({'\\$':''}, regex = True)\n",
    "df_reduced['X4']=df_reduced['X4'].replace({',':''}, regex = True)\n",
    "df_reduced['X4']=df_reduced['X4'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X5']=df_reduced['X5'].replace({'\\$':''}, regex = True)\n",
    "df_reduced['X5']=df_reduced['X5'].replace({',':''}, regex = True)\n",
    "df_reduced['X5']=df_reduced['X5'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X7']=df_reduced['X7'].str.slice(1,3)\n",
    "df_reduced['X7'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'36': 1, '60': 0}\n",
    "df_reduced['X7']=df_reduced['X7'].map(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'A': 1, 'B': 2,'C':3, 'D': 4, 'E': 5,'F':6,'G':7}\n",
    "df_reduced['X8']=df_reduced['X8'].map(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X11'] = df_reduced['X11'].str.split(expand=True)[0]\n",
    "df_reduced.replace({'X11': '<'}, 0, inplace=True)\n",
    "df_reduced.replace({'X11': '10+'}, 10, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X11'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X11'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X11'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'VERIFIED - income': 1, 'VERIFIED - income source': 1,'not verified':0}\n",
    "df_reduced['X14']=df_reduced['X14'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_reduced['X17'].unique()))\n",
    "print(df_reduced['X17'].isnull().sum())\n",
    "temp = df_reduced['X17'].value_counts().to_dict()\n",
    "for key, num in zip(temp,range(1,len(temp)+1)):\n",
    "    temp[key] = num\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X17']=df_reduced['X17'].map(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X30']=df_reduced['X30'].replace({'%':''}, regex = True)\n",
    "df_reduced['X30']=df_reduced['X30'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced['X30']=df_reduced['X30'].mask(df_reduced['X30']<=30.,1)\n",
    "df_reduced['X30']=df_reduced['X30'].mask(df_reduced['X30']>30,2)\n",
    "df_reduced['X30']=df_reduced['X30'].mask(df_reduced['X30']>50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'w': 1, 'f': 0}\n",
    "df_reduced['X32']=df_reduced['X32'].map(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliner Treatment\n",
    "sns.distplot(df_reduced['X13'])\n",
    "plt.show()\n",
    "df_reduced['X13'].plot.box(figsize=(16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_reduced.shape)\n",
    "df_reduced.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill using the mean of the feature.\n",
    "df_reduced['X30'].fillna(df_reduced['X30'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1=df_reduced['X11'].dropna()\n",
    "dt1=dt1.astype(float)\n",
    "dt1.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill using the mean of the feature.\n",
    "df_reduced['X11'].fillna(dt1.mean(),inplace=True)\n",
    "df_reduced['X11']=df_reduced['X11'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applied a more conservative approach to drop the rest of nulls due to the total numbers of null from the total population. \n",
    "df_reduced.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(12,10))\n",
    "corr=df_reduced.corr()\n",
    "sns.heatmap(corr,annot=True,fmt='.2f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced=df_reduced.drop(['X5'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Cleaning/Prepping - Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('Loan_Prediction/Holdout for Testing.csv')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=df_test[['X4','X7','X8','X11',\n",
    "        'X13', 'X14','X17', 'X21',\n",
    "       'X22','X24', 'X27', 'X28', 'X29', 'X30', 'X31',\n",
    "       'X32']]\n",
    "print(df_test.head(5))\n",
    "print(df_test.shape)\n",
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X4']=df_test['X4'].replace({'\\$':''}, regex = True)\n",
    "df_test['X4']=df_test['X4'].replace({',':''}, regex = True)\n",
    "df_test['X4']=df_test['X4'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X7']=df_test['X7'].str.slice(1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'36': 1, '60': 0}\n",
    "df_test['X7']=df_test['X7'].map(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = {'A': 1, 'B': 2,'C':3, 'D': 4, 'E': 5,'F':6,'G':7}\n",
    "df_test['X8']=df_test['X8'].map(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X11'] = df_test['X11'].str.split(expand=True)[0]\n",
    "df_test.replace({'X11': '<'}, 0, inplace=True)\n",
    "df_test.replace({'X11': '10+'}, 10, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'VERIFIED - income': 1, 'VERIFIED - income source': 1,'not verified':0}\n",
    "df_test['X14']=df_test['X14'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X17']=df_test['X17'].map(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X30']=df_test['X30'].replace({'%':''}, regex = True)\n",
    "df_test['X30']=df_test['X30'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X30']=df_test['X30'].mask(df_test['X30']<=30.,1)\n",
    "df_test['X30']=df_test['X30'].mask(df_test['X30']>30,2)\n",
    "df_test['X30']=df_test['X30'].mask(df_test['X30']>50,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = {'w': 1, 'f': 0}\n",
    "df_test['X32']=df_test['X32'].map(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outliner Treatment for X11 and X13\n",
    "sns.distplot(df_test['X13'])\n",
    "plt.show()\n",
    "df_test['X13'].plot.box(figsize=(16,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X13_New']=np.log(df_test['X13'])\n",
    "df_test.drop('X13',1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X30'].fillna(df_test['X30'].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test['X11'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=df_test['X11'].dropna()\n",
    "dt=dt.astype(float)\n",
    "print(dt.mode())\n",
    "dt.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['X11'].fillna(dt.mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['X4', 'X7', 'X8', 'X11', 'X14', 'X17', 'X21', 'X22', 'X24',\n",
    "       'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X13_New']\n",
    "df_test[cols] = df_test[cols].applymap(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting data into the following models:\n",
    "\n",
    "1. Linear Regression\n",
    "2. Lasso\n",
    "3. Ridge\n",
    "4. Decision Tree\n",
    "5. Pipelining\n",
    "6. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "import statsmodels.api as sm \n",
    "import matplotlib.lines as mlines\n",
    "import matplotlib.transforms as mtransforms\n",
    "import random\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_reduced.drop('X1',1)\n",
    "y = df_reduced['X1']\n",
    "#create x and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = PolynomialFeatures()\n",
    "features_poly = p.fit_transform(X)\n",
    "poly_df = pd.DataFrame(features_poly, columns=p.get_feature_names())\n",
    "#creating a data frame of the fit/transformed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_poly_test = p.transform(df_test)\n",
    "poly_df_test = pd.DataFrame(features_poly_test, columns=p.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(poly_df, y, random_state=28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train)\n",
    "X_train_sc = ss.transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_test_data = ss.transform(poly_df_test)\n",
    "#scaling the test data and setting it equal to a variable, scaled_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_log = np.log(y_train)\n",
    "y_test_log = np.log(y_test)\n",
    "print(X_train.shape)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lr.fit(X_train_sc, y_train_log))\n",
    "#fitting my training data to the linear regression model\n",
    "print(lr.score(X_train_sc, y_train_log))\n",
    "#scoring my training data\n",
    "lr.score(X_test_sc, y_test_log)\n",
    "#scoring my testing data r squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = RidgeCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ridge.fit(X_train_sc, y_train_log))\n",
    "#fitting my training data to the ridge model\n",
    "print(ridge.score(X_train_sc, y_train_log))\n",
    "#scoring my training data\n",
    "ridge.score(X_test_sc, y_test_log)\n",
    "#scoring my testing data r squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = LassoCV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lasso.fit(X_train_sc, y_train_log))\n",
    "#fitting my training data to the lasso model\n",
    "print(lasso.score(X_train_sc, y_train_log))\n",
    "#scoring my training data\n",
    "lasso.score(X_test_sc, y_test_log)\n",
    "#scoring my testing data r squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lasso.predict(scaled_test_data)\n",
    "#creating a variable, y_pred, and setting it equal to the lasso prediction of my scaled test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = np.exp(y_pred)\n",
    "y_pred_2 = np.exp(y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,10))\n",
    "sns.distplot(np.exp(df_reduced['X1']), kde=False, color='#A15EDB', bins=50, label='y')\n",
    "sns.distplot(y_pred_2, kde=False, color='#69547C', bins=50, label='yhat')\n",
    "\n",
    "plt.xlabel('loan rate', fontsize=19, labelpad=11)\n",
    "plt.xticks(fontsize=14)\n",
    "plt.ylabel('Count', fontsize=19, labelpad=11)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "#comparing the distribution of my target variable versus my predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "pred_df['Loan Rate'] = y_pred_1\n",
    "pred_df.to_csv('final_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree and Cross Validation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import DecisionTreeRegressor from sklearn.tree\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import sklearn.model_selection as cv\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "(X_train1, X_test1, y_train1, y_test1) = cv.train_test_split(X, y, test_size=.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train1)\n",
    "X_train_sc1 = ss.transform(X_train1)\n",
    "X_test_sc1 = ss.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate dt\n",
    "dt = DecisionTreeRegressor(max_depth=8,\n",
    "                           min_samples_leaf=0.13,\n",
    "                           random_state=3)\n",
    "\n",
    "# Fit to the training set\n",
    "dt.fit(X_train_sc1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Compute y_pred\n",
    "y_pred = dt.predict(X_test_sc1)\n",
    "\n",
    "# Compute mse_dt\n",
    "mse_dt = MSE(y_test1, y_pred)\n",
    "\n",
    "# Compute rmse_dt\n",
    "rmse_dt = mse_dt**(1/2)\n",
    "\n",
    "# Print rmse_dt\n",
    "print(\"Test set RMSE of dt: {:.2f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "# Compute the array containing the 10-folds CV MSEs\n",
    "MSE_CV_scores = - cross_val_score(dt, X_train_sc1, y_train1, cv=10, \n",
    "                                  scoring='neg_mean_squared_error', \n",
    "                                  n_jobs=-1) \n",
    "\n",
    "# Compute the 10-folds CV RMSE\n",
    "RMSE_CV = (MSE_CV_scores.mean())**(1/2)\n",
    "\n",
    "# Print RMSE_CV\n",
    "print('CV RMSE: {:.2f}'.format(RMSE_CV))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import mean_squared_error from sklearn.metrics as MSE\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "# Fit dt to the training set\n",
    "dt.fit(X_train_sc1, y_train1)\n",
    "\n",
    "# Predict the labels of the training set\n",
    "y_pred_train = dt.predict(X_train_sc1)\n",
    "\n",
    "# Evaluate the training set RMSE of dt\n",
    "RMSE_train = (MSE(y_train1, y_pred_train))**(1/2)\n",
    "\n",
    "# Print RMSE_train\n",
    "print('Train RMSE: {:.2f}'.format(RMSE_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dt.score(X_train_sc1, y_train1))\n",
    "print(dt.score(X_test_sc1, y_test1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_test = ss.transform(df_test)\n",
    "y_pred = dt.predict(ss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame()\n",
    "pred_df['Loan Rate'] = y_pred\n",
    "pred_df.to_csv('final_submission2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipelining -Combining GridSearchCV and Ridge Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param={\"step2__alpha\":[0.01,0.1,1,10,100]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "pipe2=Pipeline([('step1',StandardScaler()),('step2',Ridge())])\n",
    "grid=GridSearchCV(pipe2,param_grid=param,cv=5)\n",
    "grid.fit(X_train,y_train)\n",
    "print(\"Best cv accuracy : \",grid.best_score_)\n",
    "print(\"Best parameter : \",grid.best_params_)\n",
    "print(\"Train score : \",grid.score(X_train,y_train))\n",
    "print(\"Test score : \",grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic imports\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train1, X_test1, y_train1, y_test1) = cv.train_test_split(X, y, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "ss.fit(X_train1)\n",
    "X_train_sc1 = ss.transform(X_train1)\n",
    "X_test_sc1 = ss.transform(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a random forests regressor 'rf'400 estimators\n",
    "regrRM = RandomForestRegressor(n_estimators=400, max_depth = 50, min_samples_split = 5,min_samples_leaf =2)\n",
    "regrRM.fit(X_train_sc1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(regrRM.score(X_train_sc1, y_train1))\n",
    "print(regrRM.score(X_test_sc1, y_test1))\n",
    "y_pred1= regrRM.predict(X_test_sc1)\n",
    "print(np.sqrt(metrics.mean_squared_error(y_test1,y_pred1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrRM.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CoeffRM1 = pd.DataFrame(columns=[\"Variable\",\"FeatureImportance\"])\n",
    "CoeffRM1[\"Variable\"]=X_train1.columns\n",
    "CoeffRM1[\"FeatureImportance\"]=regrRM.feature_importances_\n",
    "CoeffRM1.sort_values(\"FeatureImportance\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set labels 'y_pred'\n",
    "y_pred_train=regrRM.predict(X_train_sc1)\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test1, y_pred1)**(1/2)\n",
    "rmse_train = MSE(y_train1, y_pred_train)**(1/2)\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "print('Train set RMSE of rf: {:.2f}'.format(rmse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try without Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train2, X_test2, y_train2, y_test2) = cv.train_test_split(X, y, test_size=.20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regrRM = RandomForestRegressor(n_estimators=400, min_samples_leaf=0.1)\n",
    "regrRM.fit(X_train2, y_train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the test set labels 'y_pred'\n",
    "y_pred2= regrRM.predict(X_test2)\n",
    "y_pred_train2=regrRM.predict(X_train2)\n",
    "# Evaluate the test set RMSE\n",
    "rmse_test = MSE(y_test2, y_pred2)**(1/2)\n",
    "rmse_train = MSE(y_train1, y_pred_train2)**(1/2)\n",
    "# Print the test set RMSE\n",
    "print('Test set RMSE of rf: {:.2f}'.format(rmse_test))\n",
    "print('Train set RMSE of rf: {:.2f}'.format(rmse_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ran 6 models for the loan predictions\n",
    "\n",
    "Linear Regression: this model returned the r-squarted of 0.90\n",
    "\n",
    "Lasso: this model also returned the r-squared of 0.90\n",
    "\n",
    "Ridge: Ridge model returned a similar r-squared of.90 comparing to linear regression model and lasso model\n",
    "\n",
    "Decision Tree: I performed the a comparsion between CV error, train error, and test error, and determined my current model is performing well as all three errors are very close (at 1.69). This model returned the r-squared of 0.85.\n",
    "\n",
    "Pipelining: I combined GridSearchCV and ridge regression model in my pipelining. This model returned the r-squared of 0.92.\n",
    "\n",
    "Random Forest Regressor: Out of all my models, the random forest regresor performed the worst due to a problem of overfitting. In order to mitigate overfitting, I set max depth to 50, the default value to 2, and min_samples_leaf to 2. But I was not able to get test RMSE and train RMSE closer. As a result, I will not use Random Forest Regrssor model to predict the loan rate.\n",
    "\n",
    "I printed out loan prediction results using Lasso (final submission) and Decision Tree methonds ( final submission 2) and validated their similarities. I will use descision tree result as my final submission as I performed my cross validation and is confident with my result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
